{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: dataset.html\n",
    "title: dataset\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import os\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class WxBSDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Wide multiple baselines stereo dataset.\"\"\"\n",
    "    urls = {\n",
    "        'v1.1': [\n",
    "            'http://cmp.felk.cvut.cz/wbs/datasets/WxBS_v1.1.zip',\n",
    "            'WxBS_v1.1.zip',\n",
    "            '66da037e7d20487e86db147307615870'\n",
    "        ]}\n",
    "    validation_pairs = ['kyiv_dolltheater2', 'petrzin']\n",
    "                        \n",
    "    def __init__(self, root_dir:str, subset: str = 'test',  version='v1.1', download: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            subset (string): can be \"val\" or \"test\".\n",
    "            download (bool): download if not found, default: False\n",
    "        \"\"\"\n",
    "        assert version in self.urls.keys()\n",
    "        self.root_dir = root_dir\n",
    "        self.subset = subset\n",
    "        self.validation_mode = self.subset == 'val'\n",
    "        self.version = version\n",
    "        self.data_dir = os.path.join(self.root_dir, version)\n",
    "        self.data_down = os.path.join(self.root_dir, '{}.zip'.format(version))\n",
    "        if not self._check_unpacked():\n",
    "            if download:\n",
    "                self.download()\n",
    "            else:\n",
    "                raise ValueError(f'Data is not in {self.data_dir}, consider set download=True')\n",
    "        self.index_dataset()\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def _check_downloaded(self):\n",
    "        if os.path.exists(self.data_down):\n",
    "            import hashlib\n",
    "            md5 = hashlib.md5(self.data_down).hexdigest()\n",
    "            if md5 == self.urls[self.version][2]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _check_unpacked(self):\n",
    "        return os.path.exists(self.data_dir)\n",
    "\n",
    "    def download(self) -> None:\n",
    "        if not self._check_downloaded():\n",
    "            # download files\n",
    "            url = self.urls[self.version][0]\n",
    "            filename = self.urls[self.version][1]\n",
    "            md5 = self.urls[self.version][2]\n",
    "            fpath = os.path.join(self.root_dir, filename)\n",
    "            tv.datasets.phototour.download_url(url, self.root_dir, filename, md5)\n",
    "            print('# Extracting data {}\\n'.format(self.data_down))\n",
    "            import zipfile\n",
    "            with zipfile.ZipFile(fpath, 'r') as z:\n",
    "                z.extractall(self.data_dir)\n",
    "            os.unlink(fpath)\n",
    "        return\n",
    "\n",
    "    def index_dataset(self):\n",
    "        sets = sorted([x for x in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, x))])\n",
    "        img_pairs_list = []\n",
    "        for s in sets:\n",
    "            if s == '.DS_Store':\n",
    "                continue\n",
    "            ss = os.path.join(self.data_dir, s)\n",
    "            pairs = os.listdir(ss)\n",
    "            for p in sorted(pairs):\n",
    "                if p == '.DS_Store':\n",
    "                    continue\n",
    "                cur_dir = os.path.join(ss, p)\n",
    "                if self.validation_mode:\n",
    "                    if p not in self.validation_pairs:\n",
    "                        continue\n",
    "                else:\n",
    "                    if p in self.validation_pairs:\n",
    "                        continue\n",
    "                if os.path.isfile(os.path.join(cur_dir, '01.png')):\n",
    "                    img_pairs_list.append((os.path.join(cur_dir, '01.png'),\n",
    "                                           os.path.join(cur_dir, '02.png'),\n",
    "                                           os.path.join(cur_dir, 'corrs.txt'),\n",
    "                                           os.path.join(cur_dir, 'crossval_errors.txt')))\n",
    "                elif os.path.isfile(os.path.join(cur_dir, '01.jpg')):\n",
    "                    img_pairs_list.append((os.path.join(cur_dir, '01.jpg'),\n",
    "                                           os.path.join(cur_dir, '02.jpg'),\n",
    "                                           os.path.join(cur_dir, 'corrs.txt'),\n",
    "                                           os.path.join(cur_dir, 'crossval_errors.txt')))\n",
    "                else:\n",
    "                    continue\n",
    "        self.pairs = img_pairs_list\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgfname1, imgfname2, pts_fname, err_fname = self.pairs[idx]\n",
    "        img1 = np.array(Image.open(imgfname1))\n",
    "        img2 = np.array(Image.open(imgfname2))\n",
    "        pts = np.loadtxt(pts_fname)\n",
    "        crossval_errors = np.loadtxt(err_fname)\n",
    "        pair_name = '/'.join(pts_fname.split('/')[-3:-1])\n",
    "        out = {'img1': img1,\n",
    "               'img2': img2,\n",
    "               'pts': pts,\n",
    "               'errors': crossval_errors,\n",
    "               'name': pair_name}\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img1', 'img2', 'pts', 'errors', 'name'])\n",
      "WGALBS/submarine2\n"
     ]
    }
   ],
   "source": [
    "dset = WxBSDataset('.WXBS', download=True)\n",
    "print (dset[10].keys())\n",
    "print (dset[10]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class EVDDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Extreme View Dataset.\"\"\"\n",
    "    urls = {\n",
    "        'v1.0': [\n",
    "            'http://cmp.felk.cvut.cz/wbs/datasets/EVD.zip',\n",
    "            'EVD.zip',\n",
    "            '7ce52151abdafa71a609424d09e43075'\n",
    "        ]}\n",
    "                        \n",
    "    def __init__(self, root_dir:str, subset: str = 'test',  version='v1.0', download: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            subset (string): can be \"val\" or \"test\".\n",
    "            download (bool): download if not found, default: False\n",
    "        \"\"\"\n",
    "        assert version in self.urls.keys()\n",
    "        self.root_dir = root_dir\n",
    "        self.subset = subset\n",
    "        self.version = version\n",
    "        self.data_dir = os.path.join(root_dir, 'EVD')\n",
    "        self.data_down = self.root_dir\n",
    "        if not self._check_unpacked():\n",
    "            if download:\n",
    "                self.download()\n",
    "            else:\n",
    "                raise ValueError(f'Data is not in {self.data_dir}, consider set download=True')\n",
    "        self.index_dataset()\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def _check_downloaded(self):\n",
    "        if os.path.exists(self.data_down):\n",
    "            import hashlib\n",
    "            md5 = hashlib.md5(self.data_down).hexdigest()\n",
    "            if md5 == self.urls[self.version][2]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _check_unpacked(self):\n",
    "        return os.path.exists(self.data_dir)\n",
    "\n",
    "    def download(self) -> None:\n",
    "        if not self._check_downloaded():\n",
    "            # download files\n",
    "            url = self.urls[self.version][0]\n",
    "            filename = self.urls[self.version][1]\n",
    "            md5 = self.urls[self.version][2]\n",
    "            fpath = os.path.join(self.root_dir, filename)\n",
    "            tv.datasets.phototour.download_url(url, self.root_dir, filename, md5)\n",
    "            print('# Extracting data {}\\n'.format(self.data_down))\n",
    "            import zipfile\n",
    "            with zipfile.ZipFile(fpath, 'r') as z:\n",
    "                z.extractall(self.root_dir)\n",
    "            os.unlink(fpath)\n",
    "        return\n",
    "\n",
    "    def index_dataset(self):\n",
    "        sets = sorted([x for x in os.listdir(os.path.join(self.data_dir, '1'))])\n",
    "        img_pairs_list = []\n",
    "        for s in sets:\n",
    "            if s == '.DS_Store':\n",
    "                continue\n",
    "            img_pairs_list.append(((os.path.join(self.data_dir, '1', s)),\n",
    "                                  (os.path.join(self.data_dir, '2', s)),\n",
    "                                  (os.path.join(self.data_dir, 'h', s.replace('png', 'txt')))))\n",
    "        self.pairs = img_pairs_list\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgfname1, imgfname2, h_fname = self.pairs[idx]\n",
    "        img1 = np.array(Image.open(imgfname1))\n",
    "        img2 = np.array(Image.open(imgfname2))\n",
    "        with open(h_fname, 'r') as hf:\n",
    "            lines = hf.readlines()\n",
    "            H = []\n",
    "            for l in lines:\n",
    "                H.append([float(x) for x in l.replace('\\t',' ').strip().split(' ') if len(x) > 0])\n",
    "            H = np.array(H)\n",
    "            H = H / H[2, 2]\n",
    "        img1_shape = img1.shape[:2]\n",
    "        img2_shape = img2.shape[:2]\n",
    "        \n",
    "        pair_name = imgfname1.split('/')[-1].split('.')[0]\n",
    "        out = {'img1': img1,\n",
    "               'img2': img2,\n",
    "               'img1_shape': img1_shape,\n",
    "               'img2_shape': img2_shape,\n",
    "               'H': H,\n",
    "               'name': pair_name}\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset returns tuples of: image1, image2, labelled GT correspondences, cross-validation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://cmp.felk.cvut.cz/wbs/datasets/EVD.zip to .EVD2/EVD.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 28531000/28531000 [00:04<00:00, 6793575.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Extracting data .EVD2\n",
      "\n",
      "dict_keys(['img1', 'img2', 'img1_shape', 'img2_shape', 'H', 'name'])\n",
      "adam\n",
      "[[ 1.60197402e-01 -9.50942558e-02  2.72132086e+02]\n",
      " [ 1.13716755e-02  8.83741135e-01  4.04505014e+00]\n",
      " [-7.88552590e-05 -3.01914075e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "dset = EVDDataset('.EVD', download=True)\n",
    "print (dset[0].keys())\n",
    "print (dset[0]['name'])\n",
    "print (dset[0]['H'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
